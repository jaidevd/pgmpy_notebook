{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Learning\n",
    "Machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data. Such algorithms operate by building a model from example inputs and using that to make predictions or decisions, rather than following strictly static program instructions. We will be considering only classification here.\n",
    "\n",
    "There are multiple ways to solve this problem:  \n",
    "1. We could find a function which can directly map an input value to it's class label. \n",
    "2. We can find the probability distributions over the variables and then use this distribution to answer queries about the new data point.\n",
    "\n",
    "In Probabilistic Graphical Models we try to do predictions using the second method. \n",
    "The most obvious way to do this would be to compute a Joint Probability Distribution over all these variables and then marginalize and reduce over these according to our new data point to get the probabilities of classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's take an example of class where we want to predict things about students. So, we have some previous data of students and for each student we have 5 features: Difficulty (D), Intelligence (I), SAT (S), Grade (G) and Recommendation Letter (L). For simplicity, we will also consider that each of these features can have only two values. So, difficulty can take two values easy and hard, Intelligence can take smart and dumb and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's now generate some random data. \n",
    "# Now let's try to compute the Joint Probability Distribution over these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now have a Joint Distribution $ P(D, I, S, G, L) $. Suppose we have a new student and we want to predict what recommendation letter he will get given that:\n",
    "* he is intelligent\n",
    "* the course was easy\n",
    "* his SAT score is good\n",
    "* he got a good grade.\n",
    "\n",
    "So, basically we want to compute $ P(L | d^0, i^1, s^1, g^1) $. \n",
    "\n",
    "From chain rule of probability we know that:\n",
    "$$ P(D, I, S, G, L) = P(L | D, I, S, G) * P(D, I, S, G) $$\n",
    "\n",
    "Thus, by expanding using the chain rule,\n",
    "$$ P(D, I, S, G, L) = P(D | I, S, G, L) * P(I | S, G, L) * P(S | G, L) * P(G | L) * P(L) $$\n",
    "\n",
    "$$ P(L | d^0, i^0, s^1, g^1) = \\frac {P(d^0, i^1, s^1, g^1, L)} {P(d^0, i^1, s^1, g^1)} $$\n",
    "\n",
    "Since we know the joint distribution, $ P(D, I, S, G, L) $ we can now easily compute both the terms $ P(d^0, i^1, s^1, g^1, L) $ and $ P(d^0, i^1, s^1, g^1) $ and using these we will be able to compute the $ P(L | d^0, i^0, s^1, g^1) $.\n",
    "\n",
    "#### TODO: Show the computed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The way we solved the problem above we had to compute the whole joint distribution to answer queries over this model. And we can see that the size of the table is exponential to the number of variables involved. So, in the case of large models this method becomes infeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic Graphical Models\n",
    "\n",
    "A Probabilistic Graphical Model (PGM) is a way of compactly representing joint probability distribution over random variables by exploiting the independence conditions of the variables. And PGM also provides us methods to easly and efficiently compute conditional probabilities over joint distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How is PGM different than other techniques?\n",
    "\n",
    "The distinctive thing about PGM is that it provides a very intuitive and natural approach for modelling complex problems along with maintaining control over the computational costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are two main types of modes in PGM:\n",
    "1. Bayesian Models: used mainly when we have causal relationship between the variables\n",
    "2. Markov Models: used when there are non causal relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the student example that we saw, we mostly have a causal relationship between variables. So, if a student is intelligent it implies that he should get good grades, getting good grades implies that he will get a better recommendation. Let's try to create a graph of dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"files/student.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network structure implies many independence conditions between the variables. Any variable in the network is independent of its non descendants, if its parents are observed. These are known as *local independencies* in Bayesian Networks. If we denote the non-descendants of the variable $X$ as $NonDesc_X$, then the local independence of $X$ can be expressed as:\n",
    "\n",
    "$$ (X \\perp NonDesc_X | Pa_X) $$\n",
    "\n",
    "Where $Pa_X$ denotes the parents of $X$.\n",
    "\n",
    "Now let's consider global independencies in the network. Connections between variables in a directed graph can only be the following four ways:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/connections_directed_graphs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of causal, evidential and common cause connections A and C are dependent if B is not observed. But if B is observed, then they become independent. Whereas in the case of common evidence A and C are independent if B is not observed but become dependent if B is observed.\n",
    "\n",
    "So in the case of our student example we can now get many dependence assertions over the network like:\n",
    "* $ L \\perp D | G $\n",
    "* $ L \\perp I | G $\n",
    "* $ D \\perp I $\n",
    "* $ D \\not\\perp I | G $, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Trail\n",
    "Two nodes in a Bayesian Network are said to be on an *active trail* if a change in one of them affects the other. An active trail can be formed in the following ways:\n",
    "* if the trail between the variables only have causal, evidential or common cause relations\n",
    "* if the trail shows a common evidence relationship.\n",
    "\n",
    "(The common evidence relation is also commonly known as V structure.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now coming back to our original joint distribution distribution:\n",
    "$$ P(D, I, G, S, R) = P(D | I, G, S, R) * P(I | G, S, R) * P(G | S, R) * P(S | R) * P(R) $$\n",
    "\n",
    "Using these independence properties in a Bayesian network we can reduce this to:\n",
    "$$ P(D, I, G, S, R) = P(D) * P(I) * P(G | D, I) * P(S | I) * P(L | G) $$\n",
    "\n",
    "And we can also notice that all the terms in this are the probabilites of each the variables given their parents in the network. These terms are known as the conditional probability distributions of the variables. Thus, computing the joint probability distribution can be broken down by computing the product of the conditional probability distributions of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example using pgmpy \n",
    "\n",
    "Let's see an example for predicting the price of a house. For simplicity we will consider that the price of the house depends only on Area, Location, Furnishing, Crime Rate and Distance from the airport. And also we will consider that all of these are discrete variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our raw data would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "raw_data = np.random.randint(low=0, high=2, size=(1000, 6))\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PGMs keep the representation of joint distributions simple by exploiting the dependencies in the variables. And these dependencies are represented using a graph. Now let's create a model for the interaction of our variables using intuition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"files/housing_price_small.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"files/housing_price.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For each node (or variable) in the network we assign a conditional probability distribution to it. The conditional probability distribution represents the probability of the variable when its parents are observed. So now using our raw data let's assign CPDs to each of these nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "import pandas as pd\n",
    "data = pd.DataFrame(raw_data, columns=['A', 'C', 'D', 'L', 'F', 'P'])\n",
    "data_train = data[: int(data.shape[0] * 0.75)]\n",
    "model = BayesianModel([('F', 'P'), ('A', 'P'), ('L', 'P'), ('C', 'L'), ('D', 'L')])\n",
    "\n",
    "model.is_active_trail('A', 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.is_active_trail('D', 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does fit do?\n",
    "The fit method adds a CPD to each node in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='files/housing_price_with_CPD.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.get_cpds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.get_cpds('P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But the data that we have for training might be baised so with pgmpy we also have the option to assign your own CPDs. Let's say the probability of getting an unfurnished home is equal to getting a furnished house. Let's adjust the values according to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pgmpy.factors import TabularCPD\n",
    "f_cpd = TabularCPD('F', 2, [[0.5], [0.5]])\n",
    "\n",
    "model.remove_cpds('F')\n",
    "model.add_cpds(f_cpd)\n",
    "\n",
    "model.check_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's try to do some reasoning on our model to verify if our intuitution for the model was correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "model = VariableElimination(model)\n",
    "# Returns a probability distribution over variables A and B.\n",
    "model.query(variables=['A', 'P'])\n",
    "print(model.query(variables=['P'], conditions={'L': {0}, 'F': {0}, 'A': {0}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "P          phi(A)  \n",
    "------------------\n",
    "P_0        0.96\n",
    "P_1        0.04\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you think about prediction about new values from this model, it is basically the same what we have been doing here. We basically ask questions about the probability of some variable giving conditions for other variables. Also we can account for missing values with just leaving it blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "test_data = data[0.75 * data.shape[0] : data.shape[0]]\n",
    "test_data.drop('P', axis=1, inplace=True)\n",
    "model.predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
